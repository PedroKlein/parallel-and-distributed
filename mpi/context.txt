#include <math.h>
#include <mpi.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "comm_strategies.h"

void sequential_matrix_multiplication(int n, double *A, double *B, double *C_sequential);
bool validate_results(int n, double *C_parallel, double *C_sequential);

void initialize_matrices(int n, double *A, double *B, double *C)
{
    for (int i = 0; i < n * n; i++)
    {
        A[i] = (double)(i % 100);
        B[i] = (double)((i % 100) + 1);
        C[i] = 0.0;
    }
}

bool verbose = false; // Make verbose available to other files

int main(int argc, char *argv[])
{
    double start_time = 0.0, end_time = 0.0;
    int rank = 0;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    if (rank == 0)
    {
        start_time = MPI_Wtime();
    }

    if (argc < 3 || argc > 5)
    {
        if (rank == 0)
        {
            fprintf(stderr, "Usage: mpirun -np <procs> %s <n> <comm_type> [--validate] [--verbose]\n", argv[0]);
            fprintf(stderr, "Communication types: collective, sync, async, async_new\n");
        }
        MPI_Finalize();
        return 1;
    }

    int size;
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int n = atoi(argv[1]);
    char *comm_type = argv[2];
    bool validation_enabled = false;

    // Parse optional flags
    for (int i = 3; i < argc; i++)
    {
        if (strcmp(argv[i], "--validate") == 0)
            validation_enabled = true;
        if (strcmp(argv[i], "--verbose") == 0)
            verbose = true;
    }

    if (n % size != 0)
    {
        if (rank == 0)
        {
            fprintf(stderr, "Error: Matrix size (n) must be divisible by the number of processes (size).\n");
        }
        MPI_Finalize();
        return 1;
    }

    double *A = NULL, *B = NULL, *C = NULL;
    int rows_per_proc = n / size;
    int elements_per_proc = n * rows_per_proc;

    B = (double *)malloc(n * n * sizeof(double));
    double *local_A = (double *)malloc(elements_per_proc * sizeof(double));
    double *local_C = (double *)malloc(elements_per_proc * sizeof(double));

    if (rank == 0)
    {
        A = (double *)malloc(n * n * sizeof(double));
        C = (double *)malloc(n * n * sizeof(double));
        initialize_matrices(n, A, B, C);
        if (verbose)
        {
            printf("[VERBOSE] Matrix size: %d x %d\n", n, n);
            printf("[VERBOSE] Number of processes: %d\n", size);
        }
    }

    if (verbose && rank == 0)
    {
        printf("[VERBOSE] Communication type: %s\n", comm_type);
    }

    if (strcmp(comm_type, "collective") == 0)
    {
        run_collective(n, rank, size, A, B, C, local_A, local_C);
    }
    else if (strcmp(comm_type, "sync") == 0)
    {
        run_sync(n, rank, size, A, B, C, local_A, local_C);
    }
    else if (strcmp(comm_type, "async") == 0)
    {
        run_async(n, rank, size, A, B, C, local_A, local_C);
    }
    else if (strcmp(comm_type, "async_new") == 0)
    {
        run_async_new(n, rank, size, A, B, C, local_A, local_C);
    }
    else
    {
        if (rank == 0)
        {
            fprintf(stderr, "Error: Invalid communication type '%s'.\n", comm_type);
        }
    }

    if (verbose && rank == 0)
    {
        end_time = MPI_Wtime();
        printf("[VERBOSE] Total execution time: %.6f seconds\n", end_time - start_time);
    }

    // --- Validation Step (Outside timing) ---
    if (rank == 0 && validation_enabled)
    {
        printf("--- Starting Validation ---\n");
        double *C_sequential = (double *)malloc(n * n * sizeof(double));
        if (C_sequential == NULL)
        {
            fprintf(stderr, "Failed to allocate memory for validation matrix.\n");
        }
        else
        {
            printf("Calculating sequential result for comparison...\n");
            sequential_matrix_multiplication(n, A, B, C_sequential);

            printf("Comparing parallel result with sequential...\n");
            validate_results(n, C, C_sequential);

            free(C_sequential);
        }
        printf("--- Validation Finished ---\n");
    }

    // Cleanup
    free(B);
    free(local_A);
    free(local_C);
    if (rank == 0)
    {
        free(A);
        free(C);
    }

    MPI_Finalize();
    return 0;
}

/**
 * @brief Computes matrix multiplication C = A * B sequentially.
 */
void sequential_matrix_multiplication(int n, double *A, double *B, double *C_sequential)
{
    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n; j++)
        {
            double sum = 0.0;
            for (int k = 0; k < n; k++)
            {
                sum += A[i * n + k] * B[k * n + j];
            }
            C_sequential[i * n + j] = sum;
        }
    }
}

/**
 * @brief Compares two matrices, C_parallel and C_sequential, element by element.
 * @return Returns true if they are equal (within a tolerance), false otherwise.
 */
bool validate_results(int n, double *C_parallel, double *C_sequential)
{
    const double epsilon = 1e-6; // tolerance for floating point errors

    for (int i = 0; i < n * n; i++)
    {
        if (fabs(C_parallel[i] - C_sequential[i]) > epsilon)
        {
            int row = i / n;
            int col = i % n;
            fprintf(stderr, "VALIDATION ERROR at position [%d][%d]!\n", row, col);
            fprintf(stderr, "  - Parallel Value:   %f\n", C_parallel[i]);
            fprintf(stderr, "  - Sequential Value: %f\n", C_sequential[i]);
            fprintf(stderr, "  - Difference:       %e\n", fabs(C_parallel[i] - C_sequential[i]));
            printf("VALIDATION FAILED.\n");
            return false;
        }
    }

    printf("VALIDATION SUCCESSFUL: The parallel result is correct.\n");
    return true;
}

#include "comm_strategies.h"
#include <mpi.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

extern bool verbose;

void run_async_new(int n, int rank, int size, double *A, double *B, double *C, double *local_A, double *local_C)
{
    int elements_per_proc = n * n / size;
    double total_start_time, total_end_time, comm_time = 0.0, comp_start_time, comp_end_time, comm_phase_start;

    MPI_Barrier(MPI_COMM_WORLD);
    total_start_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    MPI_Request bcast_req;
    if (rank == 0)
    {
        MPI_Request *send_requests = (MPI_Request *)malloc((size - 1) * sizeof(MPI_Request));
        for (int i = 1; i < size; i++)
        {
            MPI_Isend(A + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD,
                      &send_requests[i - 1]);
        }
        for (int i = 0; i < elements_per_proc; i++)
            local_A[i] = A[i];
        MPI_Ibcast(B, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD, &bcast_req);
        MPI_Wait(&bcast_req, MPI_STATUS_IGNORE);
        free(send_requests); // Free send requests, no longer need to be monitored by rank 0
    }
    else
    {
        MPI_Request recv_a_req;
        MPI_Irecv(local_A, elements_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &recv_a_req);
        MPI_Ibcast(B, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD, &bcast_req);
        MPI_Request all_requests[] = {recv_a_req, bcast_req};
        MPI_Waitall(2, all_requests, MPI_STATUSES_IGNORE);
    }
    comm_time += MPI_Wtime() - comm_phase_start;

    comp_start_time = MPI_Wtime();
    for (int i = 0; i < n / size; i++)
    {
        for (int j = 0; j < n; j++)
        {
            local_C[i * n + j] = 0.0;
            for (int k = 0; k < n; k++)
            {
                local_C[i * n + j] += local_A[i * n + k] * B[k * n + j];
            }
        }
    }
    comp_end_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    if (rank == 0)
    {
        for (int i = 0; i < elements_per_proc; i++)
            C[i] = local_C[i];
        MPI_Request *recv_requests = (MPI_Request *)malloc((size - 1) * sizeof(MPI_Request));
        for (int i = 1; i < size; i++)
        {
            MPI_Irecv(C + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 1, MPI_COMM_WORLD,
                      &recv_requests[i - 1]);
        }
        MPI_Waitall(size - 1, recv_requests, MPI_STATUSES_IGNORE);
        free(recv_requests);
    }
    else
    {
        MPI_Request send_req;
        MPI_Isend(local_C, elements_per_proc, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, &send_req);
        MPI_Wait(&send_req, MPI_STATUS_IGNORE);
    }
    comm_time += MPI_Wtime() - comm_phase_start;

    total_end_time = MPI_Wtime();

    if (rank == 0)
    {
        double total_time = total_end_time - total_start_time;
        double comp_time = comp_end_time - comp_start_time;
        if (verbose)
        {
            printf("[VERBOSE] CSV Output: comm_type=async_new, matrix_size=%d, num_procs=%d, total_time=%.6f, "
                   "comm_time=%.6f, comp_time=%.6f\n",
                   n, size, total_time, comm_time, comp_time);
        }
        else
        {
            printf("async_new,%d,%d,%.6f,%.6f,%.6f\n", n, size, total_time, comm_time, comp_time);
        }
    }
}

#include "comm_strategies.h"
#include <mpi.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

extern bool verbose;

void run_async(int n, int rank, int size, double *A, double *B, double *C, double *local_A, double *local_C)
{
    int elements_per_proc = n * n / size;
    double total_start_time, total_end_time, comm_time = 0.0, comp_start_time, comp_end_time, comm_phase_start;
    MPI_Request request;

    MPI_Barrier(MPI_COMM_WORLD);
    total_start_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    if (rank == 0)
    {
        for (int i = 1; i < size; i++)
        {
            MPI_Isend(A + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &request);
        }
        for (int i = 0; i < elements_per_proc; i++)
        {
            local_A[i] = A[i];
        }
    }
    else
    {
        // Receive part of A and wait immediately.
        MPI_Irecv(local_A, elements_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &request);
        MPI_Wait(&request, MPI_STATUS_IGNORE);
    }

    // Broadcast B and wait immediately.
    MPI_Ibcast(B, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD, &request);
    MPI_Wait(&request, MPI_STATUS_IGNORE);
    comm_time += MPI_Wtime() - comm_phase_start;

    comp_start_time = MPI_Wtime();
    for (int i = 0; i < n / size; i++)
    {
        for (int j = 0; j < n; j++)
        {
            local_C[i * n + j] = 0.0;
            for (int k = 0; k < n; k++)
            {
                local_C[i * n + j] += local_A[i * n + k] * B[k * n + j];
            }
        }
    }
    comp_end_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    if (rank == 0)
    {
        for (int i = 0; i < elements_per_proc; i++)
        {
            C[i] = local_C[i];
        }
        // Receive from each worker and wait inside the loop.
        for (int i = 1; i < size; i++)
        {
            MPI_Irecv(C + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, &request);
            MPI_Wait(&request, MPI_STATUS_IGNORE);
        }
    }
    else
    {
        // Send the result and wait immediately.
        MPI_Isend(local_C, elements_per_proc, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, &request);
        MPI_Wait(&request, MPI_STATUS_IGNORE);
    }
    comm_time += MPI_Wtime() - comm_phase_start;
    total_end_time = MPI_Wtime();

    if (rank == 0)
    {
        double total_time = total_end_time - total_start_time;
        double comp_time = comp_end_time - comp_start_time;
        if (verbose)
        {
            printf("[VERBOSE] CSV Output: comm_type=async, matrix_size=%d, num_procs=%d, total_time=%.6f, "
                   "comm_time=%.6f, comp_time=%.6f\n",
                   n, size, total_time, comm_time, comp_time);
        }
        else
        {
            printf("async,%d,%d,%.6f,%.6f,%.6f\n", n, size, total_time, comm_time, comp_time);
        }
    }
}

#include "comm_strategies.h"
#include <mpi.h>
#include <stdbool.h>
#include <stdio.h>

extern bool verbose;

void run_collective(int n, int rank, int size, double *A, double *B, double *C, double *local_A, double *local_C)
{
    int elements_per_proc = n * n / size;
    double total_start_time, total_end_time, comm_time = 0.0, comp_start_time, comp_end_time, comm_phase_start;

    MPI_Barrier(MPI_COMM_WORLD);
    total_start_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    MPI_Scatter(A, elements_per_proc, MPI_DOUBLE, local_A, elements_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(B, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    comm_time += MPI_Wtime() - comm_phase_start;

    comp_start_time = MPI_Wtime();
    for (int i = 0; i < n / size; i++)
    {
        for (int j = 0; j < n; j++)
        {
            local_C[i * n + j] = 0.0;
            for (int k = 0; k < n; k++)
            {
                local_C[i * n + j] += local_A[i * n + k] * B[k * n + j];
            }
        }
    }
    comp_end_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    MPI_Gather(local_C, elements_per_proc, MPI_DOUBLE, C, elements_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    comm_time += MPI_Wtime() - comm_phase_start;

    total_end_time = MPI_Wtime();

    if (rank == 0)
    {
        double total_time = total_end_time - total_start_time;
        double comp_time = comp_end_time - comp_start_time;
        if (verbose)
        {
            printf("[VERBOSE] CSV Output: comm_type=collective, matrix_size=%d, num_procs=%d, total_time=%.6f, "
                   "comm_time=%.6f, comp_time=%.6f\n",
                   n, size, total_time, comm_time, comp_time);
        }
        else
        {
            printf("collective,%d,%d,%.6f,%.6f,%.6f\n", n, size, total_time, comm_time, comp_time);
        }
    }
}

#include "comm_strategies.h"
#include <mpi.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>

extern bool verbose;

void run_sync(int n, int rank, int size, double *A, double *B, double *C, double *local_A, double *local_C)
{
    int elements_per_proc = n * n / size;
    double total_start_time, total_end_time, comm_time = 0.0, comp_start_time, comp_end_time, comm_phase_start;

    MPI_Barrier(MPI_COMM_WORLD);
    total_start_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    if (rank == 0)
    {
        for (int i = 1; i < size; i++)
        {
            MPI_Send(A + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
        }
        for (int i = 0; i < elements_per_proc; i++)
            local_A[i] = A[i];
    }
    else
    {
        MPI_Recv(local_A, elements_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
    MPI_Bcast(B, n * n, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    comm_time += MPI_Wtime() - comm_phase_start;

    comp_start_time = MPI_Wtime();
    for (int i = 0; i < n / size; i++)
    {
        for (int j = 0; j < n; j++)
        {
            local_C[i * n + j] = 0.0;
            for (int k = 0; k < n; k++)
            {
                local_C[i * n + j] += local_A[i * n + k] * B[k * n + j];
            }
        }
    }
    comp_end_time = MPI_Wtime();

    comm_phase_start = MPI_Wtime();
    if (rank == 0)
    {
        for (int i = 0; i < elements_per_proc; i++)
            C[i] = local_C[i];
        for (int i = 1; i < size; i++)
        {
            MPI_Recv(C + i * elements_per_proc, elements_per_proc, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
    }
    else
    {
        MPI_Send(local_C, elements_per_proc, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);
    }
    comm_time += MPI_Wtime() - comm_phase_start;

    total_end_time = MPI_Wtime();

    if (rank == 0)
    {
        double total_time = total_end_time - total_start_time;
        double comp_time = comp_end_time - comp_start_time;
        if (verbose)
        {
            printf("[VERBOSE] CSV Output: comm_type=sync, matrix_size=%d, num_procs=%d, total_time=%.6f, "
                   "comm_time=%.6f, comp_time=%.6f\n",
                   n, size, total_time, comm_time, comp_time);
        }
        else
        {
            printf("sync,%d,%d,%.6f,%.6f,%.6f\n", n, size, total_time, comm_time, comp_time);
        }
    }
}